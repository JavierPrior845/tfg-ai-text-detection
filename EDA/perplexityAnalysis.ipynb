{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b22341",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "DATASET_PATH = Path(\"dataset/multimodal_dataset.jsonl\")\n",
    "MODEL_ID = \"gpt2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf1ae2a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_perplexity(text, model, tokenizer, device):\n",
    "    \"\"\"Calcula la perplejidad de un texto usando GPT-2.\"\"\"\n",
    "    if not text or len(str(text).strip()) < 10:\n",
    "        return None\n",
    "    \n",
    "    # GPT-2 tiene un límite de 1024, pero para noticias 512 suele bastar y es más rápido\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=input_ids)\n",
    "        loss = outputs.loss  # Negative Log Likelihood\n",
    "        \n",
    "    return torch.exp(loss).item()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
